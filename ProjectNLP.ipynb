{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import csv\n",
    "from scipy.spatial.distance import cosine, cdist\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "path='D:\\\\College\\\\Semester 9\\\\NLP\\\\Lab\\\\FinalProject\\\\Tweets'\n",
    "ps = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "DocsTerms={}\n",
    "stop_words=set(stopwords.words('english')) \n",
    "i=1\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files: # looping only over first 20,000 files\n",
    "        \n",
    "        with open(os.path.join(root,file), 'r', encoding=\"utf8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if not row[0] in DocsTerms :\n",
    "                   # CheckifRetweet(row[10])\n",
    "                    if \" RT \" not in row[10] or \" rt \" not in row[10] and  len(row[10]) > 20 : \n",
    "                        key =row[0] \n",
    "                        DocsTerms[key]=[]\n",
    "                        DocsTerms[key].append(row[1])\n",
    "                        LowerVocab=row[10].lower()\n",
    "                        noLinkSent=re.sub(r'http\\S+','',LowerVocab,flags=re.MULTILINE)\n",
    "                        words = tokenizer.tokenize(noLinkSent)\n",
    "                        stemmedVocab=[]\n",
    "                        for word in words:\n",
    "                            stemmedVocab.append(ps.stem(word))\n",
    "                        filtered_sent=[w for w in stemmedVocab if not w in stop_words]\n",
    "                        DocsTerms[key].append(filtered_sent)\n",
    "\n",
    "DocsTerms.pop('tweet_id')\n",
    "\n",
    "\n",
    "train={}\n",
    "test={}\n",
    "i=0\n",
    "for key, value in DocsTerms.items():\n",
    "    if i<=11712 :\n",
    "        train[key]=[]\n",
    "        train[key].append(value[0])\n",
    "        train[key].append(value[1])\n",
    "    else:\n",
    "        test[key]=[]\n",
    "        test[key].append(value[0])\n",
    "        test[key].append(value[1])\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "TfidfVectorizerFunc = TfidfVectorizer()\n",
    "CountVectorizerFunc = CountVectorizer()\n",
    "sentences = []\n",
    "twitterId=[]\n",
    "semitment=[]\n",
    "for key, value in train.items():\n",
    "    s=\"\"\n",
    "    twitterId.append(key)\n",
    "    semitment.append(value[0])\n",
    "    for word in  value[1]:\n",
    "        s+=word+\" \"\n",
    "    sentences.append(s)\n",
    "    \n",
    "\n",
    "tfidf = TfidfVectorizerFunc.fit_transform(sentences)\n",
    "\n",
    "sentences_test = []\n",
    "twitterId_test=[]\n",
    "semitment_test=[]\n",
    "\n",
    "for key, value in test.items():\n",
    "    s=\"\"\n",
    "    twitterId_test.append(key)\n",
    "    semitment_test.append(value[0])\n",
    "    for word in  value[1]:\n",
    "        s+=word+\" \"\n",
    "    sentences_test.append(s)\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cats = ['neutral','positive','negative']\n",
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(tfidf, semitment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tfidf_test = TfidfVectorizerFunc.transform(sentences_test)\n",
    "predictions = clf.predict(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727994227994228\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(semitment_test, predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7297979797979797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(tfidf, semitment) \n",
    "predictions_neigh = neigh.predict(tfidf_test)\n",
    "print(metrics.f1_score(semitment_test, predictions_neigh, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7784992784992785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasmi\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomClassifier = RandomForestClassifier(random_state=0)\n",
    "randomClassifier.fit(tfidf, semitment) \n",
    "predictions_randomClassifier = randomClassifier.predict(tfidf_test)\n",
    "print(metrics.f1_score(semitment_test, predictions_randomClassifier, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def CheckifRetweet(tweetText):\n",
    "    regexp = re.compile(r'\\b[R|r]t\\b')\n",
    "    if regexp.search(tweetText):\n",
    "        print(\"yes\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
